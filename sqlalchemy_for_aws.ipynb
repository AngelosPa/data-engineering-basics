{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter four - SQL Alchemy for storing our data.\n",
    "Now that we have weather, flights, and demographic data in Pandas Data frames it's time to fill up some tables in MySQL. For simplicity, I chose the SQLAlchemy library to connect Python to any SQL.\n",
    "\n",
    "Create a connection in your MySQL Workbench or any database design tool of choice and create an empty database there as well. I will use the database name \"guns\" and the table names \"arrivals\", \"broadcast\" and \"cities\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "aws_access_link = os.getenv('AWS_ACCESS_LINK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import democraphic\n",
    "cities = democraphic.cities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weather_broadcast\n",
    "broadcast = weather_broadcast.top_cities_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flight_api_get_next_day_arrival\n",
    "arrivals = flight_api_get_next_day_arrival.all_cities_arrival_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we declare the variables that we need to establish the connection.\n",
    "\n",
    "You can upload them to the cloud using a provider of your choice (AWS in my example) or just save them locally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"gans\"   # name of the database you want to use here\n",
    "# to connect to your local server\n",
    "host = aws_access_link  # replace with your host name or IP address. Usually 127.0.0.1 or localhost\n",
    "user = \"admin\" # replace with your username, usually admin or root if u work locally \n",
    "password = db_password  # your password! keep it save in .env file\n",
    "port = 3306 # the port number of your database server. Usually 3306\n",
    "con = f'mysql+pymysql://{user}:{password}@{host}:{port}/{schema}' # this is the connection string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With \"to_sql\" method we can create the tables and fill them with the data from the Pandas data frames that we created above.\n",
    "\n",
    "if_exists='append' means that if the table already exists it will append the data to the table.\n",
    "\n",
    "if_exists='replace' means that if the table already exists it will replace the table with the updated data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities.to_sql('cities',          # table name;\n",
    "              if_exists='append',  \n",
    "              con=con,           # connection string\n",
    "              index=False)\n",
    "\n",
    "arrivals.to_sql('arrivals', \n",
    "                if_exists='append',\n",
    "                con=con,\n",
    "                index=False)\n",
    "broadcast.to_sql('broadcast',\n",
    "                     if_exists='append',\n",
    "                        con=con,\n",
    "                        index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have now a database with all the data that you need to establish your business. The arrivals and broadcast data will update your SQL tables with fresh information every time that you run your script. Why not automate the data pipeline by running the function once per day so you always have an updated version of the weather and flight data? The Lambda function in the AWS provider will be the right tool for that. Once your script is there and you have a RDS instance on AWS with all the tables in your project, you can just use the \"EventBridge\" service to call the Lambda script once per day, and you will have a fresh database with all the data you need.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5d44d20471fed6b31c84e96a507e39677b7979bf00486c2e6552218c91082f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
